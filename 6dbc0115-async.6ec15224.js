(("undefined"!=typeof globalThis?globalThis:self)["makoChunk_@ant-design/x"]=("undefined"!=typeof globalThis?globalThis:self)["makoChunk_@ant-design/x"]||[]).push([["6dbc0115"],{"6dbc0115":function(e,n,t){"use strict";t.d(n,"__esModule",{value:!0}),t.d(n,"texts",{enumerable:!0,get:function(){return s;}}),t("3ae5d1e9");let s=[{value:"This guide introduces how to integrate Qwen model services into applications built with Ant Design X.",paraId:0},{value:'Qwen\'s model inference service supports "OpenAI compatible mode". See official documentation: ',paraId:1},{value:"Alibaba Cloud - Qwen",paraId:1},{value:"How to get baseURL - ",paraId:2,tocIndex:0},{value:"https://help.aliyun.com/zh/model-studio/getting-started/what-is-model-studio",paraId:2,tocIndex:0},{value:"How to get API Key - ",paraId:2,tocIndex:0},{value:"https://help.aliyun.com/zh/model-studio/developer-reference/get-api-key",paraId:2,tocIndex:0},{value:"Model list - ",paraId:2,tocIndex:0},{value:"https://help.aliyun.com/zh/model-studio/getting-started/models",paraId:2,tocIndex:0},{value:"It refers to model inference services that keep the API design and usage consistent with OpenAI. This means developers can use the same code and methods as calling OpenAI models to call these compatible services, reducing integration development costs.",paraId:3,tocIndex:1},{value:"Using URL to integrate Model is a basic capability provided by X SDK. For details, see ",paraId:4,tocIndex:2},{value:"X SDK",paraId:5,tocIndex:2},{value:".",paraId:4,tocIndex:2},{value:"Note: \u{1F525} ",paraId:6,tocIndex:5},{value:"dangerouslyAllowBrowser",paraId:6,tocIndex:5},{value:" has security risks. See the official openai-node ",paraId:6,tocIndex:5},{value:"documentation",paraId:6,tocIndex:5},{value:" for details.",paraId:6,tocIndex:5},{value:"import { Bubble, BubbleListProps, Sender } from '@ant-design/x';\nimport {\n  AbstractXRequestClass,\n  OpenAIChatProvider,\n  SSEFields,\n  useXChat,\n  XModelMessage,\n  XModelParams,\n  XRequestOptions,\n} from '@ant-design/x-sdk';\nimport { Flex } from 'antd';\nimport OpenAI from 'openai';\nimport React, { useState } from 'react';\n\ntype OutputType = Partial<Record<SSEFields, any>>;\ntype InputType = XModelParams;\n\nclass OpenAiRequest<\n  Input extends InputType = InputType,\n  Output extends OutputType = OutputType,\n> extends AbstractXRequestClass<Input, Output> {\n  client: any;\n  stream: OpenAI | undefined;\n\n  _isTimeout = false;\n  _isStreamTimeout = false;\n  _isRequesting = false;\n\n  constructor(baseURL: string, options: XRequestOptions<Input, Output>) {\n    super(baseURL, options);\n    this.client = new OpenAI({\n      baseURL: 'https://dashscope.aliyuncs.com/compatible-mode/v1',\n      apiKey: 'OPENAI_API_KEY',\n      dangerouslyAllowBrowser: true,\n    });\n  }\n  get asyncHandler(): Promise<any> {\n    return Promise.resolve();\n  }\n  get isTimeout(): boolean {\n    return this._isTimeout;\n  }\n  get isStreamTimeout(): boolean {\n    return this._isStreamTimeout;\n  }\n  get isRequesting(): boolean {\n    return this._isRequesting;\n  }\n  get manual(): boolean {\n    return true;\n  }\n  async run(input: Input): Promise<void> {\n    const { callbacks } = this.options;\n    try {\n      await this.client.responses.create({\n        model: 'qwen-plus',\n        messages: input?.messages || [],\n        stream: true,\n      });\n\n      // Please implement stream data update logic based on response\n    } catch (error: any) {\n      callbacks?.onError(error);\n    }\n  }\n  abort(): void {\n    // Please implement abort based on OpenAI\n  }\n}\n\nconst provider = new OpenAIChatProvider<XModelMessage, InputType, OutputType>({\n  request: new OpenAiRequest('OPENAI', {}),\n});\n\nconst Demo: React.FC = () => {\n  const [content, setContent] = useState('');\n  const { onRequest, messages, isRequesting, abort } = useXChat({\n    provider,\n    requestPlaceholder: () => {\n      return {\n        content: 'loading...',\n        role: 'assistant',\n      };\n    },\n    requestFallback: (_, { error }) => {\n      if (error.name === 'AbortError') {\n        return {\n          content: 'Request is aborted',\n          role: 'assistant',\n        };\n      }\n      return {\n        content: error?.toString(),\n        role: 'assistant',\n      };\n    },\n  });\n\n  const items = messages.map(({ message, id }) => ({\n    key: id,\n    ...message,\n  }));\n\n  const role: BubbleListProps['role'] = {\n    assistant: {\n      placement: 'start',\n    },\n    user: { placement: 'end' },\n  };\n\n  return (\n    <Flex\n      vertical\n      justify=\"space-between\"\n      style={{\n        height: 400,\n        padding: 16,\n      }}\n    >\n      <Bubble.List role={role} items={items} />\n      <Sender\n        value={content}\n        onChange={setContent}\n        loading={isRequesting}\n        onCancel={abort}\n        onSubmit={(val) => {\n          onRequest({\n            messages: [{ role: 'user', content: val }],\n          });\n          setContent('');\n        }}\n      />\n    </Flex>\n  );\n};\n\nexport default Demo;\n",paraId:7,tocIndex:5}];}}]);
//# sourceMappingURL=6dbc0115-async.6ec15224.js.map